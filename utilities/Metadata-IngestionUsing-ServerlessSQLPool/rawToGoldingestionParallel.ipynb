{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "// Definiing pipeline parameters\r\n",
        "val ADFRunID = \"\"\r\n",
        "val loadType = \"\"\r\n",
        "val processingLayer = \"\"\r\n",
        "val System = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\r\n",
        "// Reading synapse pipeline parameter values\r\n",
        "val adfRunID = ADFRunID\r\n",
        "val consumptionType = loadType\r\n",
        "val LayerProcessing = processingLayer\r\n",
        "val SRC_System = System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\r\n",
        "import scala.concurrent.Future\r\n",
        "import scala.concurrent.ExecutionContext.Implicits.global\r\n",
        "import scala.concurrent.Await\r\n",
        "import scala.concurrent.duration._\r\n",
        "import org.apache.hadoop.fs.{FileSystem, Path}\r\n",
        "import scala.collection.mutable.ArrayBuffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        },
        "collapsed": false
      },
      "source": [
        "%%spark\r\n",
        "var metadata : String = \"\"\r\n",
        "if (LayerProcessing.toLowerCase() == \"gold\"){\r\n",
        "    metadata = spark.sql(s\"\"\"SELECT \r\n",
        "                        TargetTableName, TargetFilePath, SRC_EntityName, PrimaryKey, TargetBaseFilePath, SourceQuery, cast(isDeltaTableRequired as Int)\r\n",
        "                        FROM metadata.entitymetadata\r\n",
        "                        WHERE isActive = 1\r\n",
        "                        AND LayerProcessing = '${LayerProcessing}'\r\n",
        "                        AND SRC_System = '${SRC_System}'\r\n",
        "                        AND SourceQuery IS NOT NULL\r\n",
        "                        \"\"\").map(_.mkString(\";\")).collectAsList.toArray.toBuffer.mkString(\"#\")\r\n",
        "} else {\r\n",
        "    metadata = spark.sql(s\"\"\"\r\n",
        "                        SELECT TargetTableName, TargetFilePath, SRC_EntityName, PrimaryKey, TargetBaseFilePath, NULL AS SourceQuery, cast(isDeltaTableRequired as Int)\r\n",
        "                        FROM metadata.entitymetadata\r\n",
        "                        WHERE ConsumptionType = '${consumptionType}'\r\n",
        "                        AND isActive = 1\r\n",
        "                        AND LayerProcessing = '${LayerProcessing}'\r\n",
        "                        AND SRC_System = '${SRC_System}'\r\n",
        "                        \"\"\").map(_.mkString(\";\")).collectAsList.toArray.toBuffer.mkString(\"#\")\r\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\r\n",
        "def parseEntities (entities: String) : ArrayBuffer[(String, String , String, String, String, String, String)] = {\r\n",
        "\r\n",
        "  var sourceTables = new ArrayBuffer[(String, String , String, String, String, String, String)]()\r\n",
        "\r\n",
        "  try {\r\n",
        "  sourceTables = entities.split(\"#\").map{ line =>\r\n",
        "                                          val lineSplit = line.split(\";\")\r\n",
        "                                          ( lineSplit(0), lineSplit(1), lineSplit(2), lineSplit(3), lineSplit(4), lineSplit(5), lineSplit(6))\r\n",
        "                                        }.to[ArrayBuffer]\r\n",
        "\r\n",
        "  return sourceTables\r\n",
        "  } catch {\r\n",
        "      case e: Exception => {\r\n",
        "          throw new Exception(\"Unable to parse input parameter for sourceTables : \" + entities.toString())\r\n",
        "          }\r\n",
        "    }\r\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\r\n",
        "val allEntities = parseEntities(metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# functions to merge data for full raw into gold tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\r\n",
        "def mergeFullRawToGold(goldTableName: String, targetFolder: String, srcEntityName: String, targetBaseFilePath: String): Future[Boolean] = Future {\r\n",
        "\r\n",
        "  val rawFilePath = targetBaseFilePath + \"/\" + targetFolder + \"/\" + srcEntityName\r\n",
        "  val goldTableWritePath = targetBaseFilePath + \"/\" + targetFolder.replace(\"raw/\",\"gold/\").replace(\"/full\",\"\")\r\n",
        "\r\n",
        "  try{\r\n",
        "    val goldDf = spark.read.parquet(rawFilePath)\r\n",
        "    goldDf.write.format(\"delta\").mode(\"overwrite\").save(goldTableWritePath)\r\n",
        "\r\n",
        "    spark.sql(\"\"\"CREATE TABLE IF NOT EXISTS \"\"\" + goldTableName + \"\"\" \r\n",
        "                USING DELTA LOCATION '\"\"\" + goldTableWritePath + \"\"\"'\r\n",
        "              \"\"\")\r\n",
        "    } catch {\r\n",
        "      case e: Exception => {\r\n",
        "        throw new Exception(\"Failed to merge raw data for entity: \" + goldTableName + \" : with exception: \" + e.toString())\r\n",
        "        }\r\n",
        "    }\r\n",
        "  true\r\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# functions to merge data for delta raw into gold tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\r\n",
        "/**\r\n",
        "  Purpose: To generate the merge condition based on the primary key\r\n",
        "  Params : goldTbl<String>\r\n",
        "           stagingTbl<String>\r\n",
        "           pKeysList<Array[String]>\r\n",
        "  Return : mergeCondition<String>\r\n",
        "*/\r\n",
        "def gererateMergeCondition(goldTbl: String, stagingTbl: String, pKeysList: Array[String]): String = {\r\n",
        "  var mergeCondition = \" ON \"\r\n",
        "  val lastPk = pKeysList.last.trim()\r\n",
        "  for (pk <- pKeysList) {\r\n",
        "    val p = pk.trim()\r\n",
        "    mergeCondition += goldTbl + \".\" + p + \" = \" + stagingTbl + \".\" + p + \" \"\r\n",
        "    if (p != lastPk) {\r\n",
        "      mergeCondition += \" AND \"\r\n",
        "    }\r\n",
        "  }\r\n",
        "  return mergeCondition\r\n",
        "}\r\n",
        "\r\n",
        "/**\r\n",
        "  Purpose: Execute the function and ingest the data into Gold table.\r\n",
        "*/\r\n",
        "\r\n",
        "def mergeDeltaRawToGold(goldTableName: String, targetFolder: String, primaryKey: String, targetBaseFilePath: String, isDeltaTableRequired: String): Future[Boolean] = Future {\r\n",
        "  try{\r\n",
        "    \r\n",
        "    val deltaViewSuffix = goldTableName.split('.').last\r\n",
        "    val baseFilePath = targetBaseFilePath\r\n",
        "    val rawFolderPath = baseFilePath + \"/\" + targetFolder\r\n",
        "    val pKeysList = primaryKey.split(\",\")\r\n",
        "    \r\n",
        "    var deltaDf = spark.read.parquet(rawFolderPath)\r\n",
        "      \r\n",
        "    val isGoldTableExists = spark.catalog.tableExists(goldTableName)\r\n",
        "    if (isGoldTableExists) {\r\n",
        "      val tblDf = spark.sql(\"select * from \" + goldTableName + \" limit 1\")\r\n",
        "      val targetTblSchema = tblDf.columns.toList\r\n",
        "      val sourceSchema = deltaDf.columns.toList\r\n",
        "\r\n",
        "      if (targetTblSchema.length != sourceSchema.length) {\r\n",
        "        println(\"Schema mismatch found for \" + goldTableName + \". Handling within the process....\")\r\n",
        "        for (attr <- targetTblSchema){\r\n",
        "            if (!sourceSchema.contains(attr)) {\r\n",
        "                deltaDf = deltaDf.withColumn(attr, lit(null))\r\n",
        "            }\r\n",
        "        }\r\n",
        "      }\r\n",
        "      deltaDf = deltaDf.selectExpr(targetTblSchema:_*)      \r\n",
        "      val deltaViewName = \"deltaView_\" + deltaViewSuffix\r\n",
        "      deltaDf.createOrReplaceTempView(deltaViewName)\r\n",
        "\r\n",
        "      println(\"Gold Table Refreshed!!!!\")\r\n",
        "\r\n",
        "      if(isDeltaTableRequired.equals(\"1\")){\r\n",
        "        val deltadataDf = spark.sql(s\"\"\"with cte as (\r\n",
        "        select *, row_number() over(partition by incidentid order by modifiedon desc) rn \r\n",
        "        from $deltaViewName\r\n",
        "        )\r\n",
        "        select *\r\n",
        "        from cte\r\n",
        "        where rn =1\"\"\")\r\n",
        "\r\n",
        "        deltadataDf.createOrReplaceTempView(deltaViewName)\r\n",
        "      }\r\n",
        "      \r\n",
        "      val mergeCondition = gererateMergeCondition(goldTableName, deltaViewName, pKeysList)\r\n",
        "      val mergeQuery = s\"\"\"MERGE INTO $goldTableName  \r\n",
        "                          USING $deltaViewName \r\n",
        "                          $mergeCondition \r\n",
        "                          WHEN MATCHED \r\n",
        "                            THEN UPDATE SET *\r\n",
        "                          WHEN NOT MATCHED \r\n",
        "                            THEN INSERT *\r\n",
        "                        \"\"\"\r\n",
        "      println(mergeQuery)\r\n",
        "      spark.sql(mergeQuery)\r\n",
        "      println(goldTableName + \" is merged with delta data\")\r\n",
        "    } else {\r\n",
        "      val goldTableWritePath = baseFilePath + \"/\" + targetFolder.replace(\"raw/\",\"gold/\").replace(\"/delta\",\"\")\r\n",
        "      deltaDf.write.format(\"delta\").\r\n",
        "                    mode(\"overwrite\").\r\n",
        "                    save(goldTableWritePath)\r\n",
        "      spark.sql(\"\"\"CREATE TABLE \"\"\" + goldTableName + \"\"\"  \r\n",
        "                  USING DELTA \r\n",
        "                  LOCATION '\"\"\" + goldTableWritePath + \"\"\"'\r\n",
        "                \"\"\")\r\n",
        "      println(goldTableName + \" is loaded with delta data\")\r\n",
        "    }\r\n",
        "\r\n",
        "    if(isDeltaTableRequired.equals(\"1\"))\r\n",
        "    {\r\n",
        "      \r\n",
        "      deltaDf.createOrReplaceTempView(\"vw_incident_delta\")\r\n",
        "      val dataDf = spark.sql(\"\"\"with cte as (\r\n",
        "          select *, row_number() over(partition by incidentid order by modifiedon desc) rn \r\n",
        "          from vw_incident_delta\r\n",
        "          )\r\n",
        "          select *\r\n",
        "          from cte\r\n",
        "          where rn =1\"\"\")\r\n",
        "\r\n",
        "      print(\"delta table is needed for \"+ goldTableName)\r\n",
        "      dataDf.write.mode(\"overwrite\").saveAsTable(goldTableName+\"_delta\")\r\n",
        "    }\r\n",
        "    true\r\n",
        "  } catch {\r\n",
        "  case e: Exception => {\r\n",
        "    throw new Exception(\"Failed to merge raw data for entity: \" + goldTableName + \" : with exception: \" + e.toString())\r\n",
        "    }\r\n",
        "  }\r\n",
        "}\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# functions to merge data latest data into derived gold tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\r\n",
        "def mergeInDerivedGold(goldTableName: String, targetFolder: String, targetBaseFilePath: String, sourceQuery: String): Future[Boolean] = Future {\r\n",
        "    val goldTableWritePath = targetBaseFilePath +\"/\"+ targetFolder\r\n",
        "\r\n",
        "    try{\r\n",
        "    val goldDf = spark.sql(sourceQuery)\r\n",
        "    goldDf.write.format(\"delta\").mode(\"overwrite\").save(goldTableWritePath)\r\n",
        "    spark.sql(\"\"\"CREATE TABLE IF NOT EXISTS \"\"\" + goldTableName + \"\"\" \r\n",
        "                USING DELTA LOCATION '\"\"\" + goldTableWritePath + \"\"\"'\r\n",
        "                \"\"\")\r\n",
        "    } catch {\r\n",
        "    case e: Exception => {\r\n",
        "        throw new Exception(\"Failed to merge data for entity: \" + goldTableName + \" : with exception: \" + e.toString())\r\n",
        "        }\r\n",
        "    }\r\n",
        "    true\r\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        },
        "collapsed": true
      },
      "source": [
        "%%spark\r\n",
        "if (consumptionType.toLowerCase() == \"full_clean\" && LayerProcessing.toLowerCase() == \"raw\") {\r\n",
        "    val createFullLoadTasks = Future.sequence( allEntities.map( k => mergeFullRawToGold(k._1, k._2, k._3, k._5)) )\r\n",
        "    val viewFullLoadResults = Await.result( createFullLoadTasks, 999 minutes )\r\n",
        "} else if (consumptionType.toLowerCase() == \"delta_clean\" && LayerProcessing.toLowerCase() == \"raw\") {\r\n",
        "    val createDeltaLoadTasks = Future.sequence( allEntities.map( k => mergeDeltaRawToGold(k._1, k._2, k._4, k._5, k._7)) )\r\n",
        "    val viewDeltaLoadResults = Await.result(createDeltaLoadTasks, 999 minutes )\r\n",
        "} else if (consumptionType == \"\" && LayerProcessing.toLowerCase() == \"gold\"){\r\n",
        "    val createDerivedgoldLoadTasks = Future.sequence( allEntities.map( k => mergeInDerivedGold(k._1, k._2, k._5, k._6)) )\r\n",
        "    val viewDerivedgoldLoadResults = Await.result(createDerivedgoldLoadTasks, 999 minutes )\r\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\r\n",
        "mssparkutils.notebook.exit(\"Notebook completed\")"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "kernelspec": {
      "name": "synapse_spark",
      "display_name": "scala"
    },
    "language_info": {
      "name": "scala"
    }
  }
}