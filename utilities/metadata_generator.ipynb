{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Script to create Metadata Table"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\r\n",
        "create table parallel.metadata(\r\n",
        "\r\n",
        "    notebookid string,\r\n",
        "\r\n",
        "    notebookname string,\r\n",
        "\r\n",
        "    dependencies string,\r\n",
        "\r\n",
        "    notebookparams string,\r\n",
        "\r\n",
        "    pipeline string,\r\n",
        "\r\n",
        "    type string,\r\n",
        "\r\n",
        "    timeout string,\r\n",
        "\r\n",
        "    retry string,\r\n",
        "\r\n",
        "    retryIntervalInSeconds string,\r\n",
        "\r\n",
        "    isactive boolean\r\n",
        "\r\n",
        ") using delta\r\n",
        "\r\n",
        "location \"/data/common/parallel/metadata\";"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Please copy all Necessary Pipeline Jsons from the Repo into Container"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mssparkutils.fs.ls(\"abfss://eehrsisynapsefs@hrdatalakedev.dfs.core.windows.net/pipeline_jsons\")"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Below Is the Command to Create Mount for the Container Where the Jsons are placed"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        " mssparkutils.fs.mount( \r\n",
        "     \"abfss://eehrsisynapsefs@hrdatalakedev.dfs.core.windows.net\", \r\n",
        "     \"/pipelinejsons\", \r\n",
        "     {\"linkedService\":\"eehrsisynapsedev-WorkspaceDefaultStorage\"} \r\n",
        " ) "
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Storing the JobId in a Variable**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a=mssparkutils.env.getJobId()"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating two Variables\r\n",
        "   **1. Path Variable is to be used in mssparkutils****\r\n",
        "   ****2. Folderpath variable to be used in os.listdir**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"synfs:/\"+a+\"/pipelinejsons/pipeline_jsons\"\r\n",
        "folderpath = \"/synfs/\"+a+\"/pipelinejsons/pipeline_jsons\""
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mssparkutils.fs.ls(path) "
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\r\n",
        "import os\r\n",
        "import csv\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "\"\"\" Using the Folderpath variable to Iterate all the Json files available in the Folder \"\"\"\r\n",
        "\r\n",
        "entries = os.listdir(folderpath)\r\n",
        "inp_data = {}\r\n",
        "reference_name = {}\r\n",
        "\r\n",
        "for entry in entries:\r\n",
        "\r\n",
        "    print(entry)\r\n",
        "\r\n",
        "    data_JSON = folderpath+\"/\"+entry\r\n",
        "    f = open(data_JSON)\r\n",
        "    data_dict = json.load(f)\r\n",
        "\r\n",
        "    inp_data['notebookid'] = []\r\n",
        "    inp_data['notebookname'] = []\r\n",
        "    inp_data['dependencies'] = []\r\n",
        "    inp_data['notebookparams'] = []\r\n",
        "    inp_data['pipeline'] = []\r\n",
        "    inp_data['type'] = []\r\n",
        "    inp_data['timeout'] = []\r\n",
        "    inp_data['retry'] = []\r\n",
        "    inp_data['retryIntervalInSeconds'] = []\r\n",
        "\r\n",
        "    for data in data_dict['properties']['activities']:\r\n",
        "        name = data['name']\r\n",
        "        if data['type'] == \"ExecutePipeline\":\r\n",
        "            reference_name.update({name: dict(dict(data).get('typeProperties'))['pipeline']['referenceName']})\r\n",
        "        if data['type'] == \"SynapseNotebook\":\r\n",
        "            reference_name.update({name: dict(dict(data).get('typeProperties'))['notebook']['referenceName']})\r\n",
        "\r\n",
        "    for data in data_dict['properties']['activities']:\r\n",
        "        params = {}\r\n",
        "        # print(data)\r\n",
        "        name = data['name']\r\n",
        "        inp_data['notebookid'].append(name)\r\n",
        "        if data['dependsOn']:\r\n",
        "            inp_data['dependencies'].append(\",\".join([dependency['activity'] for dependency in data['dependsOn'] ]))\r\n",
        "        else:\r\n",
        "            inp_data['dependencies'].append('')\r\n",
        "        if data['type'] == \"ExecutePipeline\":\r\n",
        "            # reference_name.update({name: dict(dict(data).get('typeProperties'))['pipeline']['referenceName']})\r\n",
        "            inp_data['notebookname'].append(dict(dict(data).get('typeProperties'))['pipeline']['referenceName'])\r\n",
        "            inp_data['type'].append('pipeline')\r\n",
        "            if 'parameters' in dict(dict(data).get('typeProperties')).keys():\r\n",
        "                for key, value in dict(dict(data).get('typeProperties'))['parameters'].items():\r\n",
        "                    params.update({key:value})\r\n",
        "            inp_data['notebookparams'].append(params if params else '')\r\n",
        "        if data['type'] == \"SynapseNotebook\":\r\n",
        "            # reference_name.update({name:dict(dict(data).get('typeProperties'))['notebook']['referenceName']})\r\n",
        "            inp_data['notebookname'].append(dict(dict(data).get('typeProperties'))['notebook']['referenceName'])\r\n",
        "            inp_data['type'].append('notebook')\r\n",
        "            if 'parameters' in dict(dict(data).get('typeProperties')).keys():\r\n",
        "                for key, value in dict(dict(data).get('typeProperties'))['parameters'].items():\r\n",
        "                    if type(value['value']) is dict:\r\n",
        "                        params.update({key: value['value']['value']})\r\n",
        "                    else:\r\n",
        "                        params.update({key:value['value']})\r\n",
        "            inp_data['notebookparams'].append(params if params else '')\r\n",
        "\r\n",
        "        if 'policy' in data.keys():\r\n",
        "            inp_data['timeout'].append(data['policy']['timeout'])\r\n",
        "            inp_data['retry'].append(data['policy']['retry'])\r\n",
        "            inp_data['retryIntervalInSeconds'].append(data['policy']['retryIntervalInSeconds'])\r\n",
        "        else:\r\n",
        "            inp_data['timeout'].append('')\r\n",
        "            inp_data['retry'].append('')\r\n",
        "            inp_data['retryIntervalInSeconds'].append('')\r\n",
        "        inp_data['pipeline'].append(entry.split('.')[0])\r\n",
        "        \r\n",
        "\r\n",
        "    df = pd.DataFrame(inp_data)\r\n",
        "    sp_df = spark.createDataFrame(df.astype(str))\r\n",
        "    sp_df.createOrReplaceTempView(\"sp_df\")\r\n",
        "    spark.sql(\"\"\"INSERT INTO parallel.metadata select * from sp_df\"\"\")\r\n"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#%%sql\r\n",
        "#CREATE table parallel.sample_metadata\r\n",
        "#select * from sp_df;"
      ],
      "outputs": [],
      "execution_count": 56,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# //   %%spark\r\n",
        "# //   //  delete data in folders recursively equivalent as above drop table doesnot clean up data\r\n",
        "# //   val fs = org.apache.hadoop.fs.FileSystem.get(sc.hadoopConfiguration)\r\n",
        "# //   if(fs.exists(new org.apache.hadoop.fs.Path(\"abfss://eehrsisynapsefs@hrdatalakedev.dfs.core.windows.net/synapse/workspaces/eehrsisynapsedev/warehouse/parallel.db/sample_metadata\"))) {\r\n",
        "# //      mssparkutils.fs.rm(\"abfss://eehrsisynapsefs@hrdatalakedev.dfs.core.windows.net/synapse/workspaces/eehrsisynapsedev/warehouse/parallel.db/sample_metadata\", true)\r\n",
        "# // }"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {}
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# //%%sql\r\n",
        "#  TRUNCATE table parallel.sample_metadata"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {},
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -- %%sql\r\n",
        "# -- drop table parallel.metadata;"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {},
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\r\n",
        "select * from parallel.metadata;"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -- %%sql\r\n",
        "# -- create table parallel.metadata as \r\n",
        "# -- select * from parallel.sample_metadata ;"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {},
        "collapsed": false
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}